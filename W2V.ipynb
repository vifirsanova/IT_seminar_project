{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W2V.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmy0eeIS4582"
      },
      "source": [
        "Автор: [Виктория Фирсанова](https://github.com/vifirsanova)\n",
        "\n",
        "Лучше открывать в Colab: \n",
        "https://colab.research.google.com/drive/1e72NUomF8DYgF9XeLvAsKA9TRZrJsWcK?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sxbjzFC4gke"
      },
      "source": [
        "# Загрузка пакетов, библиотек и дополнительных файлов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXU8tdqXeyV7",
        "outputId": "bc90b236-ee73-4e19-f0b3-31382e474242"
      },
      "source": [
        "# цветной вывод\n",
        "!pip install colorama"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1kXzussaJGb"
      },
      "source": [
        "import json\n",
        "import random\n",
        "from colorama import Fore, Style\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import gensim"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_u3JuMlkN_n",
        "outputId": "9930bf22-c856-4931-c32e-914823d6076e"
      },
      "source": [
        "# список стоп-слов\n",
        "!wget https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 06:49:05--  https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6229 (6.1K) [text/plain]\n",
            "Saving to: ‘stopwords-ru.txt’\n",
            "\n",
            "\rstopwords-ru.txt      0%[                    ]       0  --.-KB/s               \rstopwords-ru.txt    100%[===================>]   6.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-26 06:49:05 (71.9 MB/s) - ‘stopwords-ru.txt’ saved [6229/6229]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXTw4mahdMkz"
      },
      "source": [
        "# Загрузка корпуса\n",
        "\n",
        "Corus\n",
        "\n",
        "Это наша обучающая выборка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GmXm-NSBCAI"
      },
      "source": [
        "!pip install corus\n",
        "\n",
        "from corus import load_lenta\n",
        "\n",
        "path = 'lenta-ru-news.csv.gz'\n",
        "records = load_lenta(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FooGSDZ_faJI"
      },
      "source": [
        "Загрузка токенизированного текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Y4n4UsCLBj",
        "outputId": "c8b21912-9137-4ebd-b4ad-a2952178cd29"
      },
      "source": [
        "with open('tokenized.txt') as f:\n",
        "    contents = f.read()\n",
        "\n",
        "print(f\"{Fore.RED}Токенизированый текст обучающей выборки:{Style.RESET_ALL}\\n\", train_tokenized)\n",
        "print(f\"{Fore.RED}Токенизированый текст проверочной выборки:{Style.RESET_ALL}\\n\", val_tokenized)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mТокенизированый текст обучающей выборки:\u001b[0m\n",
            " [['австрия', 'не', 'представила', 'доказательств', 'вины', 'российских', 'биатлонистов', 'австрийские', 'правоохранительные', 'органы', 'не', 'представили', 'доказательств', 'нарушения', 'российскими', 'биатлонистами', 'антидопинговых', 'правил', 'об', 'этом', 'сообщил', 'посол', 'россии', 'в', 'вене', 'дмитрий', 'любинский', 'по', 'итогам', 'встречи', 'уполномоченного', 'адвоката', 'дипмиссии', 'с', 'представителями', 'прокуратуры', 'страны', 'передает', 'тасс', 'действует', 'презумпция', 'невиновности', 'каких', 'либо', 'ограничений', 'свободы', 'передвижения', 'для', 'команды', 'нет', 'добавили', 'в', 'посольстве', 'международный', 'союз', 'биатлонистов', 'также', 'не', 'будет', 'применять', 'санкции', 'к', 'российским', 'биатлонистам', 'все', 'они', 'продолжат', 'выступление', 'на', 'кубке', 'мира', 'полиция', 'нагрянула', 'в', 'отель', 'сборной', 'россии', 'в', 'хохфильцене', 'вечером', 'декабря', 'как', 'написал', 'биатлонист', 'александр', 'логинов', 'их', 'считают', 'виновными', 'в', 'махинациях', 'с', 'переливанием', 'крови', 'биатлонисту', 'антону', 'шипулину', 'также', 'попавшему', 'в', 'список', 'полиция', 'нанесла', 'отдельный', 'визит', 'сейчас', 'он', 'тренируется', 'отдельно', 'в', 'австрийском', 'обертиллахе', 'обвинения', 'спортсмен', 'назвал', 'бредом', 'а', 'также', 'указал', 'на', 'охоту', 'на', 'ведьм', 'в', 'мировом', 'биатлоне', 'в', 'австрии', 'прием', 'допинга', 'уголовное', 'преступление', 'максимальное', 'наказание', 'за', 'его', 'употребление', 'три', 'года', 'тюрьмы', 'спорт', 'зимние', 'виды']]\n",
            "\u001b[31mТокенизированый текст проверочной выборки:\u001b[0m\n",
            " [['в', 'петербурге', 'стремительно', 'растет', 'число', 'госпитализаций', 'с', 'covid', '19', 'в', 'петербурге', 'наблюдается', 'резкий', 'рост', 'числа', 'госпитализаций', 'пациентов', 'с', 'covid', '19', 'как', 'сообщает', 'пресс', 'служба', 'смольного', '20', 'октября', 'в', 'больницы', 'петербурга', 'было', 'госпитализировано', '587', 'человек', 'с', 'коронавирусом', 'сутками', 'ранее', '499', 'в', 'первый', 'день', 'этой', 'недели', 'в', 'инфекционные', 'стационары', 'попали', '396', 'человек', 'для', 'сравнения', 'месяц', 'назад', 'в', 'петербурге', 'в', 'сутки', 'госпитализировали', '200', '350', 'человек', 'с', 'covid', '19', 'как', 'следует', 'из', 'перечня', 'эпидемиологических', 'показателей', 'всего', 'на', 'лечении', 'в', 'городских', 'стационарах', 'с', 'covid', '19', 'и', 'пневмониями', 'находится', '7', '639', 'человек', 'из', 'них', '707', 'человек', 'в', 'реанимации', 'больше', 'половины', 'подключены', 'к', 'ивл', 'растёт', 'и', 'число', 'выявленных', 'случаев', 'заражения', 'коронавирусом', 'по', 'данным', 'федерального', 'оперштаба', '20', 'октября', 'в', 'петербурге', 'выявлено', '3', '280', 'новых', 'случаев', 'covid', '19', 'неделю', 'назад', 'в', 'городе', 'выявляли', '2', '345', 'новых', 'заражений', 'за', 'сутки', 'а', 'месяц', 'назад', '1060', 'за', 'минувшие', 'сутки', 'по', 'данным', 'оперштаба', 'в', 'петербурге', 'от', 'ковида', 'скончались', '68', 'человек', 'за', 'весь', 'период', 'пандемии', 'от', 'инфекции', 'погибли', '23', '162', 'петербуржца', 'эпидемиологическая', 'ситуация', 'ухудшается', 'по', 'всей', 'стране', 'за', 'последние', 'сутки', 'в', 'россии', 'выявили', '36', '339', 'случаев', 'заражения', 'covid', '19', 'что', 'стало', 'новым', 'абсолютным', 'рекордом', 'с', 'начала', 'пандемии', 'из', 'за', 'роста', 'заболеваемости', 'коронавирусом', 'губернатор', 'петербурга', 'александр', 'беглов', 'ввёл', 'qr', 'коды', 'для', 'посещения', 'общественных', 'мест', 'с', '15', 'ноября', 'они', 'будут', 'требоваться', 'в', 'фитнес', 'центрах', 'бассейнах', 'театрах', 'кинотеатрах', 'а', 'с', '1', 'декабря', 'для', 'прохода', 'в', 'заведения', 'общепита'], ['в', 'петербурге', 'установили', 'рекорд', 'по', 'числу', 'госпитализированных', 'за', 'сутки', 'за', 'последние', 'сутки', 'в', 'петербургские', 'больницы', 'поступили', '587', 'зараженных', 'коронавирусом', 'это', 'максимальное', 'число', 'граждан', 'с', 'начала', 'четвертой', 'волны', 'пандемии', 'согласно', 'данным', 'пресс', 'службы', 'смольного', 'накануне', 'госпитализировали', '499', 'человек', 'предыдущий', 'рекорд', 'был', 'установлен', 'в', 'середине', 'октября', '14', 'числа', 'тогда', 'зафиксировали', '555', 'госпитализаций', 'за', 'сутки', 'после', 'этого', 'показатели', 'начали', 'снижаться', 'со', 'вчерашнего', 'дня', '20', 'октября', 'суточное', 'количество', 'госпитализаций', 'вновь', 'приблизилось', 'к', '500', 'зафиксировали', '499', 'новых', 'пациентов', 'во', 'время', 'третьей', 'волны', 'коронавируса', 'этим', 'летом', 'в', 'больницы', 'петербурга', 'ежедневно', 'поступали', '800', '850', 'заболевших', 'covid', '19', 'сообщали', 'в', 'пресс', 'службе', 'комитета', 'по', 'здравоохранению', 'петербурга', 'в', 'июне', 'всего', 'на', 'лечении', 'в', 'стационарах', 'сейчас', 'находятся', 'более', '7', '5', 'тыс', 'петербуржцев', 'с', 'covid', '19', 'или', 'пневмонией', 'из', 'них', '372', 'человека', 'подключены', 'к', 'ивл', 'а', '707', 'находятся', 'в', 'реанимации'], ['петербуржцы', 'в', 'очередях', 'идут', 'на', 'рекорды', 'вакцинации', 'после', 'объявления', 'о', 'qr', 'кодах', 'темпы', 'вакцинации', 'от', 'коронавируса', 'в', 'петербурге', 'на', 'второй', 'день', 'после', 'объявления', 'о', 'qr', 'кодах', 'продолжают', 'ускоряться', 'за', 'минувшую', 'среду', '20', 'октября', 'в', 'городе', 'привито', 'первым', 'компонентом', 'вакцины', '16', '503', 'человека', 'вместе', 'с', 'повторной', 'вакцинацией', 'выполнено', '22', '270', 'инъекций', 'сообщили', 'в', 'комздраве', '21', 'октября', 'в', 'первый', 'день', '19', 'октября', 'как', 'писала', 'фонтанка', 'вместе', 'со', 'вторым', 'компонентом', 'и', 'повторной', 'вакцинацией', 'было', 'сделано', '19', '132', 'инъекции', 'тогда', 'как', 'еще', 'в', 'прошлый', 'вторник', 'прививались', 'по', '9', '488', 'человек', 'в', 'сутки', 'ажиотаж', 'вокруг', 'вакцинации', 'нарастает', 'в', 'крупнейшем', 'в', 'городе', 'прививочном', 'пункте', 'в', 'тц', 'галерея', 'утром', '21', 'октября', 'собрались', 'десятки', 'горожан'], ['в', 'петербурге', 'после', 'объявления', 'о', 'qr', 'кодах', 'за', 'один', 'день', 'вакцинировались', '19', 'тысяч', 'человек', 'смольный', 'подвел', 'итоги', 'вакцинации', 'за', 'вторник', 'первый', 'день', 'после', 'сообщения', 'о', 'введении', 'qr', 'кодов', 'в', 'петербурге', 'привились', 'первым', 'компонентом', 'вакцины', '13', '624', 'человека', 'вместе', 'со', 'вторым', 'компонентом', 'и', 'повторной', 'вакцинацией', 'было', 'сделано', '19', '132', 'инъекции', 'в', 'прошлый', 'вторник', 'первым', 'компонентом', 'за', 'сутки', 'привились', '9', '488', 'человек', 'сообщили', 'в', 'комитете', 'по', 'здравоохранению', '20', 'октября', 'как', 'писала', 'фонтанка', 'в', 'первый', 'же', 'день', 'в', 'петербурге', 'встали', 'очереди', 'за', 'уколами', 'в', 'некоторых', 'тц', 'а', 'желающие', 'оборвали', 'телефоны', 'колл', 'центра', '122', 'для', 'записи', 'но', 'их', 'ждали', 'только', 'немецкие', 'танцы', 'бетховена'], ['между', 'сциллой', 'локдауна', 'и', 'харибдой', 'qr', 'кодов', 'как', 'будет', 'жить', 'петербург', 'во', 'время', 'ноябрьских', 'нерабочих', 'в', 'смольном', 'пока', 'не', 'готовы', 'назвать', 'правила', 'по', 'которым', 'город', 'будет', 'жить', 'в', 'период', 'нерабочих', 'дней', 'но', 'на', 'то', 'что', 'среди', 'рассматриваемых', 'вариантов', 'жесткий', 'московский', 'закрыть', 'почти', 'все', 'и', 'мягкий', 'краснодарский', 'доступ', 'по', 'qr', 'кодам', 'намекнул', 'глава', 'ситуационного', 'центра', 'по', 'соблюдению', 'коронавирусных', 'стандартов', 'безопасности', 'александр', 'ситов', 'в', 'ходе', 'круглого', 'стола', 'организованного', 'деловой', 'россией', '21', 'октября', 'он', 'предложил', 'бизнесу', 'поразмыслить', 'над', 'альтернативой', 'полностью', 'остановить', 'работу', 'или', 'принимать', 'клиентов', 'по', 'qr', 'кодам', 'скажите', 'что', 'для', 'вас', 'лучше', 'спросил', 'он', 'в', 'чате', 'с', 'бизнесом', 'сферы', 'horeca', 'созданном', 'еще', 'во', 'время', 'первого', 'локдауна', 'заместитель', 'главы', 'центра', 'развития', 'и', 'поддержки', 'предпринимательства', 'георгий', 'елецких', 'предложил', 'предпринимателям', 'пока', 'руководствоваться', 'разъяснением', 'минпромторга', 'в', 'котором', 'говорится', 'что', 'во', 'время', 'нерабочих', 'дней', 'работать', 'можно', 'только', 'промышленным', 'предприятиям', 'непрерывного', 'цикла', 'продуктовым', 'и', 'аптекам', 'но', 'по', 'информации', 'фонтанки', 'окончательное', 'решение', 'чиновники', 'не', 'приняли', 'и', 'анализируют', 'опыт', 'других', 'регионов', 'обсуждая', 'различные', 'варианты', 'с', 'предпринимателями', 'хороших', 'вариантов', 'нет', 'утверждают', 'последние']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXAAXF5n-OvE"
      },
      "source": [
        "# Построение словаря"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHnxBzWskUF-",
        "outputId": "e99a61de-c075-464c-a1db-584baf74446d"
      },
      "source": [
        "# стоп-слова \n",
        "stop_words = []\n",
        "\n",
        "with open('stopwords-ru.txt', encoding='utf-8') as f:\n",
        "    for word in f:\n",
        "      stop_words.append(word[:-1])\n",
        "\n",
        "\n",
        "def build_vocab(data_tokenized, pad_word=None, stop_words=stop_words):\n",
        "    freq_dict = defaultdict(int) # количество словоупотреблений в документах \n",
        "    doc_n = len(data_tokenized) # общее количество документов\n",
        "    print(f\"{Fore.RED}Количество документов:{Style.RESET_ALL} {doc_n}\\n\")\n",
        "\n",
        "    # посчитать количество словоупотреблений в документах \n",
        "    for text in data_tokenized:\n",
        "        unique_text_tokens = set(text)\n",
        "        for token in unique_text_tokens:\n",
        "            freq_dict[token] += 1\n",
        "\n",
        "\n",
        "    # удалить стоп-слова\n",
        "    for word in list(freq_dict.keys()):\n",
        "        if word in stop_words:\n",
        "            freq_dict.pop(word)\n",
        "\n",
        "\n",
        "    # удалить уникальные слова\n",
        "    freq_dict = {word: count for word, count in freq_dict.items() if count > 1}\n",
        "\n",
        "    # отсортировать слова по убыванию частоты\n",
        "    sorted_freq_dict = sorted(freq_dict.items(),\n",
        "                                reverse=True,\n",
        "                                key=lambda x: x[1])\n",
        "\n",
        "    # паддинг\n",
        "    padded_freq_dict = [(pad_word, 0)] + sorted_freq_dict\n",
        "    \n",
        "    # обновить нумерацию слов\n",
        "    word2id = {word: i for i, (word, _) in enumerate(padded_freq_dict)}\n",
        "\n",
        "    # преобразуем частотный словарь к виду матриц чисел\n",
        "    word2freq = np.array([count / doc_n for _, count in padded_freq_dict], dtype='float32')\n",
        "\n",
        "    return word2id, word2freq\n",
        "\n",
        "\n",
        "# строим словарь для обучающей выборки\n",
        "vocabulary, word_doc_freq = build_vocab(train_tokenized, pad_word='<PAD>', stop_words=stop_words)\n",
        "\n",
        "print(f\"{Fore.RED}Размер словаря:{Style.RESET_ALL} {len(vocabulary)}\\n\")\n",
        "print(f\"{Fore.RED}Образец словаря{Style.RESET_ALL}\\n\", list(vocabulary.items())[:10])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mКоличество документов:\u001b[0m 11002\n",
            "\n",
            "\u001b[31mРазмер словаря:\u001b[0m 67976\n",
            "\n",
            "\u001b[31mОбразец словаря\u001b[0m\n",
            " [('<PAD>', 0), ('сообщает', 1), ('россии', 2), ('словам', 3), ('октября', 4), ('ранее', 5), ('заявил', 6), ('ноября', 7), ('сша', 8), ('отметил', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1A0TY-u-VZz"
      },
      "source": [
        "# Векторизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDwj7b2DC4It",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e17b31-a83c-424c-ca23-bb921a78494e"
      },
      "source": [
        "def texts2ids(data_tokenized, word2id):\n",
        "    return [[word2id[token] for token in text if token in word2id]\n",
        "            for text in data_tokenized]\n",
        "\n",
        "# конвертация\n",
        "train_token_ids = texts2ids(train_tokenized, vocabulary)\n",
        "val_token_ids = texts2ids(val_tokenized, vocabulary)\n",
        "\n",
        "print(f\"{Fore.RED}Образец токенизированного текста{Style.RESET_ALL}\\n\", \n",
        "      ' '.join(train_tokenized[0][:5]))\n",
        "print(f\"{Fore.RED}Образец числовых представлений для этого текста{Style.RESET_ALL}\\n\", \n",
        "      ''.join(str(train_token_ids[0][:3])))\n",
        "n1, n2, n3 = train_token_ids[0][0], train_token_ids[0][1], train_token_ids[0][2]\n",
        "print(f\"{Fore.RED}Расшифровка с помощью словаря{Style.RESET_ALL}\\n\", list(vocabulary.items())[n1], \n",
        "      list(vocabulary.items())[n2], list(vocabulary.items())[n3])"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mОбразец токенизированного текста\u001b[0m\n",
            " австрия не представила доказательств вины\n",
            "\u001b[31mОбразец числовых представлений для этого текста\u001b[0m\n",
            " [8859, 3050, 2247]\n",
            "\u001b[31mРасшифровка с помощью словаря\u001b[0m\n",
            " ('австрия', 8859) ('представила', 3050) ('доказательств', 2247)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWs8fNOtJOms"
      },
      "source": [
        "# Паддинг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mEOB-dODEZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e554569-b074-42e2-9540-7fa8984fdc54"
      },
      "source": [
        "# паддинг последовательностей (текстов):\n",
        "# мы задаем длину n для последовательностей обучающих данных,\n",
        "# обрезаем все тексты, которые превышают эту длину\n",
        "# и заполняем нулями все тексты, длина которых ниже заданной длины\n",
        "\n",
        "class PaddedSequenceDataset(Dataset):\n",
        "    def __init__(self, data, targets, out_len=100, pad_value=0):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.out_len = out_len\n",
        "        self.pad_value = pad_value\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def ensure_length(self, txt, out_len, pad_value):\n",
        "        if len(txt) < out_len:\n",
        "            txt = list(txt) + [pad_value] * (out_len - len(txt))\n",
        "        else:\n",
        "            txt = txt[:out_len]\n",
        "        return txt\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        txt = self.data[item]\n",
        "        txt = self.ensure_length(txt, self.out_len, self.pad_value)\n",
        "        txt = torch.tensor(txt, dtype=torch.long)\n",
        "        target = torch.tensor(self.targets[item], dtype=torch.long)\n",
        "\n",
        "        return txt, target\n",
        "\n",
        "\n",
        "# максимальная длина последовательности\n",
        "MAX_SENTENCE_LEN = 50\n",
        "\n",
        "# применить паддинг\n",
        "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
        "                                      np.zeros(len(train_token_ids)),\n",
        "                                      out_len=MAX_SENTENCE_LEN)\n",
        "val_dataset = PaddedSequenceDataset(val_token_ids,\n",
        "                                     np.zeros(len(val_token_ids)),\n",
        "                                     out_len=MAX_SENTENCE_LEN)\n",
        "\n",
        "print(f\"{Fore.RED}Образец числовых представлений для текста из выборки{Style.RESET_ALL}\\n\", \n",
        "      ''.join(str(train_token_ids[0])))\n",
        "print(f\"{Fore.RED}\\nТо же после применения паддинга{Style.RESET_ALL}\\n\", train_dataset[0])"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mОбразец числовых представлений для текста из выборки\u001b[0m\n",
            " [8859, 3050, 2247, 5460, 49, 13882, 18411, 1613, 663, 2892, 2247, 760, 1365, 7872, 797, 16, 4310, 2, 8510, 132, 200, 326, 13077, 2846, 7317, 1957, 1412, 12, 26, 64, 2015, 17024, 1139, 250, 3469, 395, 10557, 341, 2218, 4413, 1441, 999, 13882, 5742, 461, 396, 28026, 4624, 1396, 12322, 174, 24620, 6261, 594, 2, 15817, 732, 30, 122, 10078, 120, 10558, 320, 6262, 13076, 22153, 1907, 32409, 22152, 32410, 48789, 375, 174, 9651, 11682, 4132, 17023, 3528, 15816, 38818, 274, 1011, 119, 24619, 589, 11683, 20092, 4311, 32408, 2538, 3097, 12321, 149, 1532, 5072, 694, 2811, 1329, 28, 1019, 598]\n",
            "\u001b[31m\n",
            "То же после применения паддинга\u001b[0m\n",
            " (tensor([ 8859,  3050,  2247,  5460,    49, 13882, 18411,  1613,   663,  2892,\n",
            "         2247,   760,  1365,  7872,   797,    16,  4310,     2,  8510,   132,\n",
            "          200,   326, 13077,  2846,  7317,  1957,  1412,    12,    26,    64,\n",
            "         2015, 17024,  1139,   250,  3469,   395, 10557,   341,  2218,  4413,\n",
            "         1441,   999, 13882,  5742,   461,   396, 28026,  4624,  1396, 12322]), tensor(0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdz6jri0JksQ"
      },
      "source": [
        "# Модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSD9C6i2EIdj"
      },
      "source": [
        "# побочная функция создает квадратную матрицу\n",
        "# потребуется для создания маски для обучения модели\n",
        "def make_diag_mask(size, radius):\n",
        "    idxs = torch.arange(size)\n",
        "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
        "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
        "    return mask\n",
        "\n",
        "\n",
        "# модель Skip Gram Negative Sampling\n",
        "class SkipGramNegativeSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, sentence_len, radius=3, negative_samples_n=30):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.negative_samples_n = negative_samples_n\n",
        "\n",
        "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.center_emb.weight.data[0] = 0\n",
        "\n",
        "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
        "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.context_emb.weight.data[0] = 0\n",
        "\n",
        "        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        batch_size = sentences.shape[0]\n",
        "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
        "\n",
        "        # оценить сходство с соседними словами\n",
        "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
        "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
        "        positive_probs = torch.sigmoid(positive_sims)\n",
        "\n",
        "        # увеличить оценку вероятности совместной встречаемости для пар слов\n",
        "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
        "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
        "                                               positive_mask.expand_as(positive_probs))\n",
        "\n",
        "        # выбрать случайные \"отрицательные\" слова\n",
        "        negative_words = torch.randint(1, self.vocab_size,\n",
        "                                       size=(batch_size, self.negative_samples_n),\n",
        "                                       device=sentences.device)  # Batch x NegSamplesN\n",
        "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
        "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
        "        \n",
        "        # уменьшить оценку вероятности совместной встречаемости для пар слов\n",
        "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
        "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
        "\n",
        "        return positive_loss + negative_loss\n",
        "\n",
        "\n",
        "# фиктивная функция потерь\n",
        "def no_loss(pred, target):\n",
        "    return pred"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmFwfUJlPkmy"
      },
      "source": [
        "# Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAFYcz7Jc8tT",
        "outputId": "f1527c16-e731-48a2-b1bf-e6d9b4dcd374"
      },
      "source": [
        "# функция для сохранения обученной модели\n",
        "def copy_data_to_device(data, device):\n",
        "    if torch.is_tensor(data):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, (list, tuple)):\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\n",
        "    raise ValueError()\n",
        "\n",
        "\n",
        "def train_and_eval(model, train_dataset, val_dataset, loss_func,\n",
        "                   lr=1e-4, epochs=10, batch_size=32,\n",
        "                   device=None, early_stopping_patience=10, l2=0,\n",
        "                   max_batches_per_epoch_train=10000,\n",
        "                   max_batches_per_epoch_val=1000,\n",
        "                   data_loader=DataLoader,\n",
        "                   optimizer=None,\n",
        "                   lr_scheduler=None,\n",
        "                   shuffle_train=True,\n",
        "                   dataloader_workers_n=0):\n",
        "    \"\"\"\n",
        "    Цикл для обучения и оценки модели\n",
        "    :model: модель для обучения (torch.nn.Module)\n",
        "    :train_dataset: обучающая выборка (torch.utils.data.Dataset)\n",
        "    :val_dataset: проверочная выборка (torch.utils.data.Dataset)\n",
        "    :loss_func: функция потерь (func)\n",
        "    :lr: скорость обучения (float)\n",
        "    :epochs: количество эпох обучения (int)\n",
        "    :batch_size: размер батча (int)\n",
        "    :device: cuda/cpu (str)\n",
        "    :early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
        "        отсутствие улучшения модели (int)\n",
        "    :l2: коэффициент L2-регуляризации (int)\n",
        "    :max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения (int)\n",
        "    :max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации (int)\n",
        "    :data_loader: преобразует датасет в батчи (torch.utils.data.DataLoader)\n",
        "    :optimizer: оптимизатор (torch.optim.optimizer)\n",
        "    :lr_scheduler: задает скорость обучения (torch.optim.lr_scheduler)\n",
        "    :shuffle_train: перемешать данные в обучающей выборке (Bool)\n",
        "    :return: среднее значение функции потерь на валидации на лучшей эпохе, лучшая модель (tuple)\n",
        "    \"\"\"\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
        "    lr_scheduler = lr_scheduler(optimizer)\n",
        "    train_dataloader = data_loader(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                        num_workers=dataloader_workers_n)\n",
        "    val_dataloader = data_loader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                      num_workers=dataloader_workers_n)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        try:\n",
        "            print(f\"Эпоха {epoch}\")\n",
        "\n",
        "            model.train()\n",
        "            mean_train_loss = 0\n",
        "            train_batches_n = 0\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_train:\n",
        "                    break\n",
        "\n",
        "                batch_x = copy_data_to_device(batch_x, device)\n",
        "                batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                pred = model(batch_x)\n",
        "                loss = loss_func(pred, batch_y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                mean_train_loss += float(loss)\n",
        "                train_batches_n += 1\n",
        "\n",
        "            mean_train_loss /= train_batches_n\n",
        "            print(f\"Mean train loss = {mean_train_loss}\")\n",
        "\n",
        "\n",
        "            # После каждой эпохи качество модели оценивается по проверочной выборке\n",
        "            model.eval()\n",
        "            mean_val_loss = 0\n",
        "            val_batches_n = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                    if batch_i > max_batches_per_epoch_val:\n",
        "                        break\n",
        "\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                    pred = model(batch_x)\n",
        "                    loss = loss_func(pred, batch_y)\n",
        "\n",
        "                    mean_val_loss += float(loss)\n",
        "                    val_batches_n += 1\n",
        "\n",
        "            mean_val_loss /= val_batches_n\n",
        "            print(f\"Mean val loss = {mean_val_loss}\")\n",
        "\n",
        "            if mean_val_loss < best_val_loss:\n",
        "                best_epoch = epoch\n",
        "                best_val_loss = mean_val_loss\n",
        "                best_model = copy.deepcopy(model)\n",
        "            elif epoch - best_epoch > early_stopping_patience:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step(mean_val_loss)\n",
        "\n",
        "            print()\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "            break\n",
        "\n",
        "    return best_val_loss, best_model\n",
        "\n",
        "model = SkipGramNegativeSampling(vocab_size=len(vocabulary), emb_size=100, sentence_len=MAX_SENTENCE_LEN, radius=3, negative_samples_n=30)\n",
        "\n",
        "best_val_loss, best_model = train_and_eval(model,\n",
        "                                           train_dataset,\n",
        "                                           val_dataset,\n",
        "                                           no_loss,\n",
        "                                           lr=1e-2,\n",
        "                                           epochs=5,\n",
        "                                           batch_size=4,\n",
        "                                           device='cpu',\n",
        "                                           early_stopping_patience=5,\n",
        "                                           max_batches_per_epoch_train=2000,\n",
        "                                           max_batches_per_epoch_val=len(val_dataset),\n",
        "                                           lr_scheduler=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0\n",
            "Mean train loss = 0.6624369698128422\n",
            "Mean val loss = 0.4005441516637802\n",
            "\n",
            "Эпоха 1\n",
            "Mean train loss = 0.41524487316816944\n",
            "Mean val loss = 0.3722411245107651\n",
            "\n",
            "Эпоха 2\n",
            "Mean train loss = 0.34845139679016324\n",
            "Mean val loss = 0.337588369846344\n",
            "\n",
            "Эпоха 3\n",
            "Mean train loss = 0.31081490734706574\n",
            "Mean val loss = 0.35516928136348724\n",
            "\n",
            "Эпоха 4\n",
            "Mean train loss = 0.2810781160737204\n",
            "Mean val loss = 0.5022161304950714\n",
            "Epoch     5: reducing learning rate of group 0 to 1.0000e-03.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qorNTaVuSOlc"
      },
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "# model.load_state_dict(torch.load('model_weights.pth'))"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Exd9kSjh-5"
      },
      "source": [
        "# Анализ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1OHQfOlIrK-"
      },
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, embeddings, word2id):\n",
        "        self.embeddings = embeddings\n",
        "        self.embeddings /= (np.linalg.norm(self.embeddings, ord=2, axis=-1, keepdims=True) + 1e-4)\n",
        "        self.word2id = word2id\n",
        "        self.id2word = {i: w for w, i in word2id.items()}\n",
        "\n",
        "\n",
        "    def similar(self, word, num=10):\n",
        "        \"\"\"\n",
        "        Функция находит наиболее похожие слова на данное (в основе - функция get_similar)\n",
        "        :word: слово для анализа (str)\n",
        "        :num: количество сходных слов для отображения (int)\n",
        "        :return: num кортежей вида ('аналогичное слово' (str), вероятность сходства (0 <= float <= 1)) (list)\n",
        "        \"\"\"\n",
        "        return self.get_similar(self.get_vector(word), num=num)\n",
        "\n",
        "\n",
        "    def get_similar(self, vec, num=10):\n",
        "        \"\"\"\n",
        "        Функция находит наиболее похожие вектора на данный\n",
        "        :vec: вектор слова для анализа (numpy.ndarray)\n",
        "        :num: количество сходных слов для отображения (int)\n",
        "        :return: num кортежей вида ('аналогичное слово' (str), вероятность сходства (0 <= float <= 1)) (list)\n",
        "        \"\"\"\n",
        "        similarities = (self.embeddings * vec).sum(-1) # сходные слова\n",
        "        top_similar = np.argpartition(-similarities, num, axis=0)[:num]\n",
        "        result = [(self.id2word[i], similarities[i]) for i in top_similar] # список кортежей\n",
        "        result.sort(key=lambda x: -x[1]) # сортировка результата по убыванию\n",
        "        return result\n",
        "\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        if word not in self.word2id:\n",
        "            raise ValueError('Неизвестное слово \"{}\"'.format(word))\n",
        "        return self.embeddings[self.word2id[word]]\n",
        "\n",
        "\n",
        "    def get_vectors(self, *words):\n",
        "        word_ids = [self.word2id[i] for i in words]\n",
        "        vectors = np.stack([self.embeddings[i] for i in word_ids], axis=0)\n",
        "        return vectors\n",
        "\n",
        "\n",
        "embeddings = Word2Vec(model.center_emb.weight.detach().cpu().numpy(), vocabulary)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7LQOCA-IubR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72627afb-d370-47ac-f879-262588796ab0"
      },
      "source": [
        "embeddings.similar('президент')"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('президент', 0.9999279),\n",
              " ('путин', 0.6271558),\n",
              " ('владимир', 0.6147858),\n",
              " ('глава', 0.56598836),\n",
              " ('аморальной', 0.5588019),\n",
              " ('дональд', 0.5536129),\n",
              " ('нурсултан', 0.5534665),\n",
              " ('бин', 0.5100548),\n",
              " ('хасан', 0.5007653),\n",
              " ('сша', 0.49176666)]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIXSWAy_xxXD",
        "outputId": "9be33612-d32e-403c-ae1f-614b7fa4c02c"
      },
      "source": [
        "embeddings.similar('рф')"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('рф', 0.99994165),\n",
              " ('ук', 0.5646295),\n",
              " ('ростуризма', 0.4952554),\n",
              " ('денис', 0.48497206),\n",
              " ('статье', 0.48457465),\n",
              " ('россии', 0.4773477),\n",
              " ('запущенного', 0.47727886),\n",
              " ('нагрузка', 0.47535282),\n",
              " ('лидеры', 0.4736279),\n",
              " ('администраций', 0.4718611)]"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lp2Zb8nI35v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "53b177e3-3c76-4f28-ad20-6149f66ee54f"
      },
      "source": [
        "clustering_words = ['тест', 'диагноз', 'заболевания', 'футбол', 'спорт', 'плавание', 'соревнования', 'победа', 'политика', 'президент', 'министр', 'финансы', 'управление', 'глава', 'мвд']\n",
        "clustering_vectors = embeddings.get_vectors(*clustering_words)\n",
        "print(\"Визуализация тематических кластеров векторов слов\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "projections = TruncatedSVD().fit_transform(clustering_vectors)\n",
        "x = projections[:, 0]\n",
        "y = projections[:, 1]\n",
        "ax.scatter(x, y)\n",
        "for cur_x, cur_y, cur_label in zip(x, y, clustering_words):\n",
        "    ax.annotate(cur_label, (cur_x, cur_y))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Визуализация тематических кластеров векторов слов\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}